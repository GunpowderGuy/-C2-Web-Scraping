{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f185456",
   "metadata": {},
   "source": [
    "\n",
    "# C2 Web Scraping — Plantilla Lista para Usar\n",
    "\n",
    "Esta plantilla está pensada para el proyecto descrito en tu ZIP. Está organizada en **dos celdas** tal como pediste:\n",
    "1) **Instalación e importación de librerías**  \n",
    "2) **Código de scraping y exportación**\n",
    "\n",
    "La celda 2 está preparada para trabajar en **dos modos**:\n",
    "- **`mode = \"web\"`**: descarga páginas en vivo (con rotación de User-Agent, reintentos y espera).  \n",
    "- **`mode = \"local\"`**: lee HTMLs guardados en una carpeta (p. ej., `pagina-guardada/`).\n",
    "\n",
    "> 🔧 **Qué debes editar** en la celda 2 (sección “CONFIG”):\n",
    "> - `CURRENT_TARGET`: Un nombre identificador (e.g., `\"tottus_arroz\"` o `\"empleos_fake\"`).\n",
    "> - `start_urls`: Lista de URLs iniciales o una sola URL de categoría.\n",
    "> - `selectors`: Diccionario con selectores CSS/XPath de `item`, `campos` y `paginación`.\n",
    "> - `local_html_dir`: Carpeta con páginas guardadas si usas `mode=\"local\"`.\n",
    ">\n",
    "> 💾 Al final, los datos se guardan en `data/{CURRENT_TARGET}.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe29534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================\n",
    "# 1) Instalación + Importaciones\n",
    "# ======================\n",
    "\n",
    "# Si estás en un entorno limpio (Colab/VSCode/Jupyter), puedes descomentar lo siguiente:\n",
    "!pip install -q requests beautifulsoup4 lxml pandas tenacity fake-useragent html5lib\n",
    "\n",
    "import os, time, re, json, math, random, sys, pathlib\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Any, Optional, Iterable\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Opcionales / robustez\n",
    "try:\n",
    "    from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "except Exception:\n",
    "    # Fallback mínimo si tenacity no está\n",
    "    retry = lambda *a, **k: (lambda f: f)\n",
    "    def stop_after_attempt(n): return None\n",
    "    def wait_exponential(multiplier=1, min=1, max=10): return None\n",
    "    def retry_if_exception_type(*a, **k): return None\n",
    "\n",
    "try:\n",
    "    from fake_useragent import UserAgent\n",
    "    _ua = UserAgent()\n",
    "    def get_ua():\n",
    "        try:\n",
    "            return _ua.random\n",
    "        except Exception:\n",
    "            return \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\"\n",
    "except Exception:\n",
    "    def get_ua():\n",
    "        # UA fijo si no hay fake_useragent\n",
    "        return \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\"\n",
    "\n",
    "# Asegurar carpeta de salida\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "print(\"✅ Librerías importadas y carpeta 'data/' lista.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6504d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================\n",
    "# 2) Scraping + Export\n",
    "# ======================\n",
    "\n",
    "# -------- CONFIG --------\n",
    "#mode = \"local\"  # \"web\" para navegar en vivo | \"local\" para leer HTML guardado\n",
    "mode = web\n",
    "#CURRENT_TARGET = \"demo_target\"  # Cambia por lo que indique tu imagen (p.ej., \"tottus_arroz\", \"empleos_fake\")\n",
    "CURRENT_TARGET = \n",
    "\n",
    "# Para modo \"web\": pon aquí la(s) URL(s) a scrapear (categoría o listado)\n",
    "start_urls = [\n",
    "    # \"https://www.ejemplo.com/categoria/arroz?page=1\"\n",
    "]\n",
    "\n",
    "# Para modo \"local\": carpeta con páginas ya guardadas (.html). Ej: \"pagina-guardada\"\n",
    "local_html_dir = \"pagina-guardada\"  # cambia a la carpeta real de tu ZIP si difiere\n",
    "\n",
    "# Selectores: edita esto según el sitio objetivo. Incluye CSS o XPaths (solo CSS por simplicidad aquí).\n",
    "# Estructura esperada:\n",
    "# - \"item\": selector del contenedor de cada elemento/registro\n",
    "# - \"fields\": dict con { nombre_campo: selector_css }\n",
    "# - \"pagination\": selector para el enlace a la siguiente página (opcional en modo \"web\")\n",
    "selectors = {\n",
    "    \"item\": \".product-card, .producto, .job-card, .resultado\",  # ejemplo general\n",
    "    \"fields\": {\n",
    "        \"titulo\": \".product-title, .title, .job-title\",\n",
    "        \"precio\": \".price, .product-price, .monto, .salary\",\n",
    "        \"detalle_url\": \"a[href]\",\n",
    "        \"categoria\": \".breadcrumb .active, .category-name\",\n",
    "        \"tienda_o_empresa\": \".brand, .company, .store-name\"\n",
    "    },\n",
    "    \"pagination\": \"a.next, a[aria-label='Siguiente'], .pagination-next a\"\n",
    "}\n",
    "\n",
    "# Umbrales y opciones\n",
    "MAX_PAGES = 30         # tope de páginas por seguridad\n",
    "REQUEST_DELAY = (1,3)  # espera aleatoria entre requests (segundos)\n",
    "TIMEOUT = 20           # timeout por request\n",
    "HEADERS = lambda: {\"User-Agent\": get_ua(), \"Accept-Language\": \"es-PE,es;q=0.9,en;q=0.8\"}\n",
    "\n",
    "# Limpieza básica por defecto (ajustable para tu caso)\n",
    "def clean_text(x: Optional[str]) -> Optional[str]:\n",
    "    if x is None: return None\n",
    "    x = re.sub(r\"\\s+\", \" \", x).strip()\n",
    "    return x if x else None\n",
    "\n",
    "# ---------------- Helpers (web) ----------------\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=10),\n",
    "       retry=retry_if_exception_type((requests.RequestException,)))\n",
    "def fetch(url: str) -> str:\n",
    "    resp = requests.get(url, headers=HEADERS(), timeout=TIMEOUT)\n",
    "    resp.raise_for_status()\n",
    "    return resp.text\n",
    "\n",
    "def parse_list_html(html: str, base_url: Optional[str]=None) -> List[Dict[str, Any]]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    items = soup.select(selectors[\"item\"])\n",
    "    rows = []\n",
    "    for it in items:\n",
    "        row = {}\n",
    "        for field, sel in selectors[\"fields\"].items():\n",
    "            el = it.select_one(sel)\n",
    "            if el:\n",
    "                if field.endswith(\"_url\"):\n",
    "                    # normalizar URL relativa\n",
    "                    href = el.get(\"href\")\n",
    "                    if href and base_url and href.startswith(\"/\"):\n",
    "                        from urllib.parse import urljoin\n",
    "                        row[field] = urljoin(base_url, href)\n",
    "                    else:\n",
    "                        row[field] = href\n",
    "                else:\n",
    "                    row[field] = clean_text(el.get_text(strip=True))\n",
    "            else:\n",
    "                row[field] = None\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def find_next_url(html: str, base_url: Optional[str]=None) -> Optional[str]:\n",
    "    if not selectors.get(\"pagination\"):\n",
    "        return None\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    nxt = soup.select_one(selectors[\"pagination\"])\n",
    "    if nxt and nxt.get(\"href\"):\n",
    "        href = nxt[\"href\"]\n",
    "        if base_url and href.startswith(\"/\"):\n",
    "            from urllib.parse import urljoin\n",
    "            return urljoin(base_url, href)\n",
    "        return href\n",
    "    return None\n",
    "\n",
    "# ---------------- Pipeline ----------------\n",
    "all_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "if mode == \"web\":\n",
    "    visited = set()\n",
    "    to_visit = list(start_urls)\n",
    "    pages = 0\n",
    "\n",
    "    while to_visit and pages < MAX_PAGES:\n",
    "        url = to_visit.pop(0)\n",
    "        if url in visited: \n",
    "            continue\n",
    "        visited.add(url)\n",
    "        pages += 1\n",
    "\n",
    "        print(f\"[{pages}] GET {url}\")\n",
    "        html = fetch(url)\n",
    "        batch = parse_list_html(html, base_url=url)\n",
    "        print(f\"  ↳ {len(batch)} registros\")\n",
    "        all_rows.extend(batch)\n",
    "\n",
    "        # Paginación\n",
    "        next_url = find_next_url(html, base_url=url)\n",
    "        if next_url and next_url not in visited:\n",
    "            to_visit.append(next_url)\n",
    "\n",
    "        time.sleep(random.uniform(*REQUEST_DELAY))\n",
    "\n",
    "elif mode == \"local\":\n",
    "    # Leer todos los .html dentro de local_html_dir (recursivo)\n",
    "    html_files = []\n",
    "    for root, _, files in os.walk(local_html_dir):\n",
    "        for fn in files:\n",
    "            if fn.lower().endswith((\".html\",\".htm\")):\n",
    "                html_files.append(os.path.join(root, fn))\n",
    "    if not html_files:\n",
    "        print(f\"⚠️ No se encontraron HTMLs en '{local_html_dir}'. Cambia la carpeta o usa mode='web'.\")\n",
    "    else:\n",
    "        print(f\"Procesando {len(html_files)} archivos HTML locales…\")\n",
    "        for i, fp in enumerate(sorted(html_files)):\n",
    "            try:\n",
    "                with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                    html = f.read()\n",
    "                batch = parse_list_html(html, base_url=None)\n",
    "                print(f\"  [{i+1}/{len(html_files)}] {os.path.basename(fp)} → {len(batch)} registros\")\n",
    "                all_rows.extend(batch)\n",
    "            except Exception as e:\n",
    "                print(f\"  [x] Error en {fp}: {e}\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"mode debe ser 'web' o 'local'\")\n",
    "\n",
    "# ---------------- Post-procesamiento + Export ----------------\n",
    "df = pd.DataFrame(all_rows).drop_duplicates()\n",
    "# Normaliza precios si vienen con símbolos (opcional)\n",
    "if \"precio\" in df.columns:\n",
    "    def normalize_price(s):\n",
    "        if pd.isna(s): return None\n",
    "        # Extrae dígitos/decimales con coma o punto\n",
    "        m = re.findall(r\"[\\d\\.,]+\", str(s))\n",
    "        if not m: return None\n",
    "        # Toma la última ocurrencia tipo \"1,234.56\" o \"1.234,56\" o \"1234\"\n",
    "        val = m[-1]\n",
    "        # Heurística para coma/punto\n",
    "        if val.count(\",\") > 0 and val.count(\".\") > 0:\n",
    "            # asume formato 1.234,56 -> reemplaza . y usa , como decimal\n",
    "            val = val.replace(\".\", \"\").replace(\",\", \".\")\n",
    "        else:\n",
    "            val = val.replace(\",\", \"\")\n",
    "        try:\n",
    "            return float(val)\n",
    "        except:\n",
    "            return None\n",
    "    df[\"precio_num\"] = df[\"precio\"].apply(normalize_price)\n",
    "\n",
    "out_csv = f\"data/{CURRENT_TARGET}.csv\"\n",
    "df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\n✅ Listo: {len(df)} filas → {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a8ffcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
