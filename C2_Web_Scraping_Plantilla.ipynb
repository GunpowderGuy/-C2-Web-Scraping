{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f185456",
   "metadata": {},
   "source": [
    "\n",
    "# C2 Web Scraping â€” Plantilla Lista para Usar\n",
    "\n",
    "Esta plantilla estÃ¡ pensada para el proyecto descrito en tu ZIP. EstÃ¡ organizada en **dos celdas** tal como pediste:\n",
    "1) **InstalaciÃ³n e importaciÃ³n de librerÃ­as**  \n",
    "2) **CÃ³digo de scraping y exportaciÃ³n**\n",
    "\n",
    "La celda 2 estÃ¡ preparada para trabajar en **dos modos**:\n",
    "- **`mode = \"web\"`**: descarga pÃ¡ginas en vivo (con rotaciÃ³n de User-Agent, reintentos y espera).  \n",
    "- **`mode = \"local\"`**: lee HTMLs guardados en una carpeta (p. ej., `pagina-guardada/`).\n",
    "\n",
    "> ðŸ”§ **QuÃ© debes editar** en la celda 2 (secciÃ³n â€œCONFIGâ€):\n",
    "> - `CURRENT_TARGET`: Un nombre identificador (e.g., `\"tottus_arroz\"` o `\"empleos_fake\"`).\n",
    "> - `start_urls`: Lista de URLs iniciales o una sola URL de categorÃ­a.\n",
    "> - `selectors`: Diccionario con selectores CSS/XPath de `item`, `campos` y `paginaciÃ³n`.\n",
    "> - `local_html_dir`: Carpeta con pÃ¡ginas guardadas si usas `mode=\"local\"`.\n",
    ">\n",
    "> ðŸ’¾ Al final, los datos se guardan en `data/{CURRENT_TARGET}.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe29534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================\n",
    "# 1) InstalaciÃ³n + Importaciones\n",
    "# ======================\n",
    "\n",
    "# Si estÃ¡s en un entorno limpio (Colab/VSCode/Jupyter), puedes descomentar lo siguiente:\n",
    "!pip install -q requests beautifulsoup4 lxml pandas tenacity fake-useragent html5lib\n",
    "\n",
    "import os, time, re, json, math, random, sys, pathlib\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Any, Optional, Iterable\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Opcionales / robustez\n",
    "try:\n",
    "    from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "except Exception:\n",
    "    # Fallback mÃ­nimo si tenacity no estÃ¡\n",
    "    retry = lambda *a, **k: (lambda f: f)\n",
    "    def stop_after_attempt(n): return None\n",
    "    def wait_exponential(multiplier=1, min=1, max=10): return None\n",
    "    def retry_if_exception_type(*a, **k): return None\n",
    "\n",
    "try:\n",
    "    from fake_useragent import UserAgent\n",
    "    _ua = UserAgent()\n",
    "    def get_ua():\n",
    "        try:\n",
    "            return _ua.random\n",
    "        except Exception:\n",
    "            return \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\"\n",
    "except Exception:\n",
    "    def get_ua():\n",
    "        # UA fijo si no hay fake_useragent\n",
    "        return \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\"\n",
    "\n",
    "# Asegurar carpeta de salida\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "print(\"âœ… LibrerÃ­as importadas y carpeta 'data/' lista.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6504d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================\n",
    "# 2) Scraping + Export\n",
    "# ======================\n",
    "\n",
    "# -------- CONFIG --------\n",
    "#mode = \"local\"  # \"web\" para navegar en vivo | \"local\" para leer HTML guardado\n",
    "mode = web\n",
    "#CURRENT_TARGET = \"demo_target\"  # Cambia por lo que indique tu imagen (p.ej., \"tottus_arroz\", \"empleos_fake\")\n",
    "CURRENT_TARGET = \n",
    "\n",
    "# Para modo \"web\": pon aquÃ­ la(s) URL(s) a scrapear (categorÃ­a o listado)\n",
    "start_urls = [\n",
    "    # \"https://www.ejemplo.com/categoria/arroz?page=1\"\n",
    "]\n",
    "\n",
    "# Para modo \"local\": carpeta con pÃ¡ginas ya guardadas (.html). Ej: \"pagina-guardada\"\n",
    "local_html_dir = \"pagina-guardada\"  # cambia a la carpeta real de tu ZIP si difiere\n",
    "\n",
    "# Selectores: edita esto segÃºn el sitio objetivo. Incluye CSS o XPaths (solo CSS por simplicidad aquÃ­).\n",
    "# Estructura esperada:\n",
    "# - \"item\": selector del contenedor de cada elemento/registro\n",
    "# - \"fields\": dict con { nombre_campo: selector_css }\n",
    "# - \"pagination\": selector para el enlace a la siguiente pÃ¡gina (opcional en modo \"web\")\n",
    "selectors = {\n",
    "    \"item\": \".product-card, .producto, .job-card, .resultado\",  # ejemplo general\n",
    "    \"fields\": {\n",
    "        \"titulo\": \".product-title, .title, .job-title\",\n",
    "        \"precio\": \".price, .product-price, .monto, .salary\",\n",
    "        \"detalle_url\": \"a[href]\",\n",
    "        \"categoria\": \".breadcrumb .active, .category-name\",\n",
    "        \"tienda_o_empresa\": \".brand, .company, .store-name\"\n",
    "    },\n",
    "    \"pagination\": \"a.next, a[aria-label='Siguiente'], .pagination-next a\"\n",
    "}\n",
    "\n",
    "# Umbrales y opciones\n",
    "MAX_PAGES = 30         # tope de pÃ¡ginas por seguridad\n",
    "REQUEST_DELAY = (1,3)  # espera aleatoria entre requests (segundos)\n",
    "TIMEOUT = 20           # timeout por request\n",
    "HEADERS = lambda: {\"User-Agent\": get_ua(), \"Accept-Language\": \"es-PE,es;q=0.9,en;q=0.8\"}\n",
    "\n",
    "# Limpieza bÃ¡sica por defecto (ajustable para tu caso)\n",
    "def clean_text(x: Optional[str]) -> Optional[str]:\n",
    "    if x is None: return None\n",
    "    x = re.sub(r\"\\s+\", \" \", x).strip()\n",
    "    return x if x else None\n",
    "\n",
    "# ---------------- Helpers (web) ----------------\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=10),\n",
    "       retry=retry_if_exception_type((requests.RequestException,)))\n",
    "def fetch(url: str) -> str:\n",
    "    resp = requests.get(url, headers=HEADERS(), timeout=TIMEOUT)\n",
    "    resp.raise_for_status()\n",
    "    return resp.text\n",
    "\n",
    "def parse_list_html(html: str, base_url: Optional[str]=None) -> List[Dict[str, Any]]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    items = soup.select(selectors[\"item\"])\n",
    "    rows = []\n",
    "    for it in items:\n",
    "        row = {}\n",
    "        for field, sel in selectors[\"fields\"].items():\n",
    "            el = it.select_one(sel)\n",
    "            if el:\n",
    "                if field.endswith(\"_url\"):\n",
    "                    # normalizar URL relativa\n",
    "                    href = el.get(\"href\")\n",
    "                    if href and base_url and href.startswith(\"/\"):\n",
    "                        from urllib.parse import urljoin\n",
    "                        row[field] = urljoin(base_url, href)\n",
    "                    else:\n",
    "                        row[field] = href\n",
    "                else:\n",
    "                    row[field] = clean_text(el.get_text(strip=True))\n",
    "            else:\n",
    "                row[field] = None\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def find_next_url(html: str, base_url: Optional[str]=None) -> Optional[str]:\n",
    "    if not selectors.get(\"pagination\"):\n",
    "        return None\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    nxt = soup.select_one(selectors[\"pagination\"])\n",
    "    if nxt and nxt.get(\"href\"):\n",
    "        href = nxt[\"href\"]\n",
    "        if base_url and href.startswith(\"/\"):\n",
    "            from urllib.parse import urljoin\n",
    "            return urljoin(base_url, href)\n",
    "        return href\n",
    "    return None\n",
    "\n",
    "# ---------------- Pipeline ----------------\n",
    "all_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "if mode == \"web\":\n",
    "    visited = set()\n",
    "    to_visit = list(start_urls)\n",
    "    pages = 0\n",
    "\n",
    "    while to_visit and pages < MAX_PAGES:\n",
    "        url = to_visit.pop(0)\n",
    "        if url in visited: \n",
    "            continue\n",
    "        visited.add(url)\n",
    "        pages += 1\n",
    "\n",
    "        print(f\"[{pages}] GET {url}\")\n",
    "        html = fetch(url)\n",
    "        batch = parse_list_html(html, base_url=url)\n",
    "        print(f\"  â†³ {len(batch)} registros\")\n",
    "        all_rows.extend(batch)\n",
    "\n",
    "        # PaginaciÃ³n\n",
    "        next_url = find_next_url(html, base_url=url)\n",
    "        if next_url and next_url not in visited:\n",
    "            to_visit.append(next_url)\n",
    "\n",
    "        time.sleep(random.uniform(*REQUEST_DELAY))\n",
    "\n",
    "elif mode == \"local\":\n",
    "    # Leer todos los .html dentro de local_html_dir (recursivo)\n",
    "    html_files = []\n",
    "    for root, _, files in os.walk(local_html_dir):\n",
    "        for fn in files:\n",
    "            if fn.lower().endswith((\".html\",\".htm\")):\n",
    "                html_files.append(os.path.join(root, fn))\n",
    "    if not html_files:\n",
    "        print(f\"âš ï¸ No se encontraron HTMLs en '{local_html_dir}'. Cambia la carpeta o usa mode='web'.\")\n",
    "    else:\n",
    "        print(f\"Procesando {len(html_files)} archivos HTML localesâ€¦\")\n",
    "        for i, fp in enumerate(sorted(html_files)):\n",
    "            try:\n",
    "                with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                    html = f.read()\n",
    "                batch = parse_list_html(html, base_url=None)\n",
    "                print(f\"  [{i+1}/{len(html_files)}] {os.path.basename(fp)} â†’ {len(batch)} registros\")\n",
    "                all_rows.extend(batch)\n",
    "            except Exception as e:\n",
    "                print(f\"  [x] Error en {fp}: {e}\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"mode debe ser 'web' o 'local'\")\n",
    "\n",
    "# ---------------- Post-procesamiento + Export ----------------\n",
    "df = pd.DataFrame(all_rows).drop_duplicates()\n",
    "# Normaliza precios si vienen con sÃ­mbolos (opcional)\n",
    "if \"precio\" in df.columns:\n",
    "    def normalize_price(s):\n",
    "        if pd.isna(s): return None\n",
    "        # Extrae dÃ­gitos/decimales con coma o punto\n",
    "        m = re.findall(r\"[\\d\\.,]+\", str(s))\n",
    "        if not m: return None\n",
    "        # Toma la Ãºltima ocurrencia tipo \"1,234.56\" o \"1.234,56\" o \"1234\"\n",
    "        val = m[-1]\n",
    "        # HeurÃ­stica para coma/punto\n",
    "        if val.count(\",\") > 0 and val.count(\".\") > 0:\n",
    "            # asume formato 1.234,56 -> reemplaza . y usa , como decimal\n",
    "            val = val.replace(\".\", \"\").replace(\",\", \".\")\n",
    "        else:\n",
    "            val = val.replace(\",\", \"\")\n",
    "        try:\n",
    "            return float(val)\n",
    "        except:\n",
    "            return None\n",
    "    df[\"precio_num\"] = df[\"precio\"].apply(normalize_price)\n",
    "\n",
    "out_csv = f\"data/{CURRENT_TARGET}.csv\"\n",
    "df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\nâœ… Listo: {len(df)} filas â†’ {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a8ffcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
