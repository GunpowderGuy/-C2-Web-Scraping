{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e823a65c",
   "metadata": {},
   "source": [
    "\n",
    "# C2 — Web Scraping (Tottus · Arroz) — Versión 2\n",
    "\n",
    "**Objetivo:** Extraer productos de la categoría **Arroz** en Tottus, guardar un **CSV** y mostrar una **EDA mínima**.  \n",
    "Incluye **manejador de errores** para evitar `EmptyDataError` cuando no se extraen productos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d096f50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Celda 1: Instalación (opcional) e Importaciones ===\n",
    "# Descomenta si necesitas instalar en tu entorno (Colab/entorno limpio):\n",
    "!pip install -q requests beautifulsoup4 lxml pandas\n",
    "\n",
    "import os, re, time, random, json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, parse_qs, urlencode, urlunparse\n",
    "\n",
    "BASE_URL = \"https://www.tottus.com.pe\"\n",
    "START_URL = \"https://www.tottus.com.pe/tottus-pe/lista/CATG16815/Arroz\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\",\n",
    "    \"Accept-Language\": \"es-PE,es;q=0.9,en;q=0.8\",\n",
    "}\n",
    "\n",
    "OUT_DIR = \"data\"\n",
    "OUT_CSV = os.path.join(OUT_DIR, \"tottus_arroz.csv\")\n",
    "OUT_JSON = os.path.join(OUT_DIR, \"tottus_arroz.json\")\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def clean_text(s):\n",
    "    if s is None: \n",
    "        return None\n",
    "    s = re.sub(r\"\\s+\", \" \", str(s)).strip()\n",
    "    return s or None\n",
    "\n",
    "def normalize_price(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    m = re.findall(r\"[\\d\\.,]+\", str(s))\n",
    "    if not m: \n",
    "        return None\n",
    "    val = m[-1]\n",
    "    # Heurística para 1.234,56 o 1,234.56 o 1234\n",
    "    if val.count(\",\") and val.count(\".\"):\n",
    "        val = val.replace(\".\", \"\").replace(\",\", \".\")\n",
    "    else:\n",
    "        val = val.replace(\",\", \"\")\n",
    "    try:\n",
    "        return float(val)\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14d60bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] GET https://www.tottus.com.pe/tottus-pe/lista/CATG16815/Arroz\n",
      "   ↳ 0 productos\n",
      "[2] GET https://www.tottus.com.pe/tottus-pe/lista/CATG16815/Arroz?page=2\n",
      "   ↳ 0 productos\n",
      "   (No se encontraron más productos, deteniendo paginación).\n",
      "\n",
      "⚠️ No se extrajeron productos. No se generará CSV vacío para evitar errores posteriores.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Celda 2: Web Scraping + Export CSV/JSON (con manejo de errores) ===\n",
    "\n",
    "def get_page(url, max_tries=3, sleep_base=1.2):\n",
    "    \"\"\"Descarga una página con pequeños reintentos y retorna el HTML en texto.\"\"\"\n",
    "    for i in range(max_tries):\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=25)\n",
    "            r.raise_for_status()\n",
    "            return r.text\n",
    "        except requests.RequestException as e:\n",
    "            if i == max_tries - 1:\n",
    "                raise\n",
    "            time.sleep(sleep_base + i)\n",
    "\n",
    "def select_first_text(node, selectors):\n",
    "    \"\"\"Prueba una lista de selectores CSS y retorna el texto del primero que exista.\"\"\"\n",
    "    for sel in selectors:\n",
    "        el = node.select_one(sel)\n",
    "        if el:\n",
    "            txt = el.get(\"title\") or el.get_text(strip=True)\n",
    "            txt = clean_text(txt)\n",
    "            if txt:\n",
    "                return txt\n",
    "    return None\n",
    "\n",
    "def select_first_href(node, selectors):\n",
    "    for sel in selectors:\n",
    "        el = node.select_one(sel)\n",
    "        if el and el.get(\"href\"):\n",
    "            return el.get(\"href\")\n",
    "    return None\n",
    "\n",
    "def parse_list(html, base=BASE_URL):\n",
    "    \"\"\"Extrae productos desde HTML de la categoría (estrategia defensiva + fallback JSON-LD).\"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    items = []\n",
    "\n",
    "    # 1) Tarjetas de producto (clases comunes VTEX/Tottus)\n",
    "    cards = soup.select('[data-component=\"productSummary\"], .product-card, .vtex-product-summary-2-x-container')\n",
    "    for c in cards:\n",
    "        title = select_first_text(\n",
    "            c, ['a[title]', '.product-title', '.vtex-product-summary-2-x-productBrand', 'h3 a']\n",
    "        )\n",
    "        price = select_first_text(\n",
    "            c, ['.price', '.sellingPrice', '.vtex-store-components-3-x-sellingPrice', 'span[class*=\"sellingPrice\"]', '[class*=\"price\"]']\n",
    "        )\n",
    "        href  = select_first_href(c, ['a[href]'])\n",
    "        url   = urljoin(base, href) if href else None\n",
    "        brand = select_first_text(c, ['.brand', '.vtex-product-summary-2-x-brandName', '[data-brand]'])\n",
    "        cat_el   = soup.select_one('nav.breadcrumb, .breadcrumb, [aria-label=\"breadcrumb\"]')\n",
    "        categ = clean_text(cat_el.get_text(\" > \", strip=True) if cat_el else \"Arroz\")\n",
    "\n",
    "        items.append({\n",
    "            \"titulo\": title,\n",
    "            \"precio\": price,\n",
    "            \"precio_num\": normalize_price(price),\n",
    "            \"detalle_url\": url,\n",
    "            \"marca\": brand,\n",
    "            \"categoria\": categ,\n",
    "            \"tienda\": \"Tottus\",\n",
    "        })\n",
    "\n",
    "    # 2) Fallback: JSON-LD (datos estructurados) si no encontró tarjetas\n",
    "    if not items:\n",
    "        for script in soup.select('script[type=\"application/ld+json\"]'):\n",
    "            try:\n",
    "                data = json.loads(script.string or \"\")\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            def as_list(x):\n",
    "                if isinstance(x, list): return x\n",
    "                elif x is not None: return [x]\n",
    "                return []\n",
    "\n",
    "            for p in as_list(data if isinstance(data, list) else [data]):\n",
    "                if isinstance(p, dict) and p.get(\"@type\") in (\"Product\", \"ItemList\"):\n",
    "                    if p.get(\"@type\") == \"ItemList\" and \"itemListElement\" in p:\n",
    "                        for it in as_list(p[\"itemListElement\"]):\n",
    "                            prod = it.get(\"item\") if isinstance(it, dict) else None\n",
    "                            if not isinstance(prod, dict): \n",
    "                                continue\n",
    "                            name = clean_text(prod.get(\"name\"))\n",
    "                            url = prod.get(\"url\")\n",
    "                            offer = prod.get(\"offers\") or {}\n",
    "                            price = clean_text(str(offer.get(\"price\"))) if isinstance(offer, dict) else None\n",
    "                            items.append({\n",
    "                                \"titulo\": name,\n",
    "                                \"precio\": price,\n",
    "                                \"precio_num\": normalize_price(price),\n",
    "                                \"detalle_url\": url,\n",
    "                                \"marca\": clean_text(prod.get(\"brand\", {}).get(\"name\") if isinstance(prod.get(\"brand\"), dict) else prod.get(\"brand\")),\n",
    "                                \"categoria\": \"Arroz\",\n",
    "                                \"tienda\": \"Tottus\",\n",
    "                            })\n",
    "                    elif p.get(\"@type\") == \"Product\":\n",
    "                        name = clean_text(p.get(\"name\"))\n",
    "                        url = p.get(\"url\")\n",
    "                        offer = p.get(\"offers\") or {}\n",
    "                        price = clean_text(str(offer.get(\"price\"))) if isinstance(offer, dict) else None\n",
    "                        items.append({\n",
    "                            \"titulo\": name,\n",
    "                            \"precio\": price,\n",
    "                            \"precio_num\": normalize_price(price),\n",
    "                            \"detalle_url\": url,\n",
    "                            \"marca\": clean_text(p.get(\"brand\", {}).get(\"name\") if isinstance(p.get(\"brand\"), dict) else p.get(\"brand\")),\n",
    "                            \"categoria\": \"Arroz\",\n",
    "                            \"tienda\": \"Tottus\",\n",
    "                        })\n",
    "\n",
    "    # Limpieza final (duplicados, nulos)\n",
    "    clean_items = []\n",
    "    seen = set()\n",
    "    for it in items:\n",
    "        key = (it.get(\"titulo\"), it.get(\"detalle_url\"))\n",
    "        if key not in seen and (it.get(\"titulo\") or it.get(\"detalle_url\")):\n",
    "            seen.add(key)\n",
    "            clean_items.append(it)\n",
    "    return clean_items\n",
    "\n",
    "def next_page_url(url, page_num):\n",
    "    \"\"\"Construye ?page=N manteniendo otros parámetros.\"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    qs = parse_qs(parsed.query)\n",
    "    qs[\"page\"] = [str(page_num)]\n",
    "    new_query = urlencode(qs, doseq=True)\n",
    "    return urlunparse((parsed.scheme, parsed.netloc, parsed.path, parsed.params, new_query, parsed.fragment))\n",
    "\n",
    "# === Ejecutar scraping con paginación simple ===\n",
    "MAX_PAGES = 8  # Ajusta si quieres más páginas\n",
    "all_rows = []\n",
    "for page in range(1, MAX_PAGES + 1):\n",
    "    url = START_URL if page == 1 else next_page_url(START_URL, page)\n",
    "    print(f\"[{page}] GET {url}\")\n",
    "    html = get_page(url)\n",
    "    batch = parse_list(html, base=BASE_URL)\n",
    "    print(f\"   ↳ {len(batch)} productos\")\n",
    "    if page > 1 and not batch:\n",
    "        print(\"   (No se encontraron más productos, deteniendo paginación).\")\n",
    "        break\n",
    "    all_rows.extend(batch)\n",
    "    time.sleep(random.uniform(1.1, 2.0))  # cortesía\n",
    "\n",
    "# Convertir a DataFrame y guardar si hay datos\n",
    "df = pd.DataFrame(all_rows).drop_duplicates(subset=[\"titulo\", \"detalle_url\"])\n",
    "\n",
    "if df.empty:\n",
    "    print(\"\\n⚠️ No se extrajeron productos. No se generará CSV vacío para evitar errores posteriores.\")\n",
    "else:\n",
    "    df.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(df.to_dict(orient=\"records\"), f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\n✅ Productos únicos: {len(df)}\")\n",
    "    print(f\"💾 CSV:  {OUT_CSV}\")\n",
    "    print(f\"💾 JSON: {OUT_JSON}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea401e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No hay CSV para leer (posiblemente no se extrajeron productos).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Celda 3: Vista previa y EDA mínima (segura) ===\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "csv_path = \"data/tottus_arroz.csv\"\n",
    "\n",
    "if not os.path.exists(csv_path) or os.path.getsize(csv_path) == 0:\n",
    "    print(\"⚠️ No hay CSV para leer (posiblemente no se extrajeron productos).\")\n",
    "else:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Vista rápida\n",
    "    display(df.head(10))\n",
    "    print(\"\\nFilas:\", len(df))\n",
    "    print(\"Columnas:\", list(df.columns))\n",
    "\n",
    "    # Resumen de precios\n",
    "    if \"precio_num\" in df.columns and df[\"precio_num\"].notna().any():\n",
    "        print(\"\\nPrecio — resumen:\")\n",
    "        display(df[\"precio_num\"].describe())\n",
    "\n",
    "    # Top marcas (si existen)\n",
    "    if \"marca\" in df.columns:\n",
    "        top_marcas = df[\"marca\"].fillna(\"Sin marca\").value_counts().head(10)\n",
    "        if not top_marcas.empty:\n",
    "            print(\"\\nTop 10 marcas por # de productos:\")\n",
    "            display(top_marcas)\n",
    "\n",
    "            # Gráfico simple\n",
    "            plt.figure()\n",
    "            top_marcas.sort_values(ascending=True).plot(kind=\"barh\")\n",
    "            plt.title(\"Top 10 marcas (conteo de productos)\")\n",
    "            plt.xlabel(\"Productos\")\n",
    "            plt.ylabel(\"Marca\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
